"""
Yerel Whisper modeli - OpenAI orijinal whisper entegrasyonu (pywhispercpp ile GGML desteÄŸi)
"""
import os
import torch
import whisper
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Any
from pywhispercpp.model import Model as WhisperCppModel
import tempfile
import subprocess
from ..utils.logger import LoggerMixin
from ..utils.config import Config
import psutil
import time
import threading

try:
    from pywhispercpp.model import Model as WhisperCppModel  # type: ignore
    PYWHISPERCPP_AVAILABLE = True
except ImportError:
    WhisperCppModel = None
    PYWHISPERCPP_AVAILABLE = False

try:
    import whisper  # type: ignore
    OPENAI_WHISPER_AVAILABLE = True
except ImportError:
    OPENAI_WHISPER_AVAILABLE = False


class LocalWhisperModel(LoggerMixin):
    """Yerel Whisper modeli sÄ±nÄ±fÄ± (OpenAI orijinal whisper + pywhispercpp ile GGML desteÄŸi)"""
    
    def __init__(self, config: Config):
        super().__init__()
        self.config = config
        self.whisper_config = config.get("whisper", {})
        self.performance_config = config.get("performance", {})
        
        # CPU kontrolÃ¼ parametreleri
        self.cpu_limit = self.performance_config.get("cpu_usage_limit", 0.9)
        self.auto_adjust_threads = self.performance_config.get("auto_adjust_threads", True)
        self.cpu_monitor_active = False
        
        # Model ayarlarÄ±
        self.model_size = self.whisper_config.get("model", "large-v3")
        self.language = self.whisper_config.get("language", "tr")
        self.task = self.whisper_config.get("task", "transcribe")
        self.temperature = self.whisper_config.get("temperature", 0.0)
        self.beam_size = self.whisper_config.get("beam_size", 5)
        self.best_of = self.whisper_config.get("best_of", 5)
        
        # GGML model path
        self.models_dir = Path("models")
        self.ggml_model_path = self.models_dir / "ggml-large-v3-turbo.bin"
        
        # GPU ayarlarÄ±
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # GPU optimizasyonlarÄ±
        if torch.cuda.is_available():
            # Mixed precision kullan
            if config.get("performance.mixed_precision", True):
                torch.set_default_dtype(torch.float16)
            
            # GPU memory optimizasyonu
            gpu_memory_fraction = config.get("performance.gpu_memory_fraction", 0.9)
            torch.cuda.set_per_process_memory_fraction(gpu_memory_fraction)
            
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        
        self.model = None
        self.log_info(f"LocalWhisperModel baÅŸlatÄ±ldÄ± - Device: {self.device}")
        self.log_info(f"GGML Model Path: {self.ggml_model_path}")
        
        # Model seÃ§im mantÄ±ÄŸÄ±
        self.use_ggml_model = False
        
        if self._check_ggml_model_exists():
            self.log_info(f"GGML Model Path: {os.path.join('models', f'ggml-{self.model_size}.bin')}")
            self.use_ggml_model = True
            self.log_info("Yerel Whisper modeli (GPU optimizeli) kullanÄ±lÄ±yor")
            self._load_model()
        else:
            existing_model = self._check_existing_whisper_models()
            if existing_model:
                self.log_info(f"Mevcut OpenAI Whisper modeli bulundu: {existing_model}")
                self.model_size = existing_model
                self._load_model()
            else:
                self.log_info("KullanÄ±labilir model bulunamadÄ±, gerektiÄŸinde yÃ¼klenecek")
    
    def _check_ggml_model_exists(self) -> bool:
        """GGML modelinin varlÄ±ÄŸÄ±nÄ± kontrol et"""
        exists = self.ggml_model_path.exists()
        if exists:
            size_mb = self.ggml_model_path.stat().st_size / (1024 * 1024)
            self.log_info(f"GGML modeli bulundu: {self.ggml_model_path} ({size_mb:.1f} MB)")
        else:
            self.log_warning(f"GGML modeli bulunamadÄ±: {self.ggml_model_path}")
        return exists

    def _check_existing_whisper_models(self) -> Optional[str]:
        """Models klasÃ¶rÃ¼ndeki mevcut Whisper modellerini kontrol et"""
        model_patterns = {
            "large-v3": ["large-v3.pt"],
            "large-v2": ["large-v2.pt"],
            "large": ["large.pt"],
            "medium": ["medium.pt"],
            "small": ["small.pt"],
            "base": ["base.pt"],
            "tiny": ["tiny.pt"]
        }
        
        # Ä°stenen model boyutuna gÃ¶re kontrol et
        size_key = self.model_size.replace("-turbo", "")  # large-v3-turbo -> large-v3
        
        if size_key in model_patterns:
            for pattern in model_patterns[size_key]:
                model_path = self.models_dir / pattern
                if model_path.exists():
                    size_mb = model_path.stat().st_size / (1024 * 1024)
                    self.log_info(f"Mevcut Whisper Pytorch modeli bulundu: {model_path} ({size_mb:.1f} MB)")
                    return str(model_path)
        
        return None

    def _load_model(self):
        """
        Whisper modelini yÃ¼kle - SADECE yerel GGML modelini kullanmaya zorla.
        Fallback veya indirme yok.
        """
        if self.model is not None:
            return

        self.log_info("Model yÃ¼kleme mantÄ±ÄŸÄ±: Sadece yerel GGML modeli kullanÄ±lacak.")

        # 1. GGML modelinin varlÄ±ÄŸÄ±nÄ± kontrol et
        if not self._check_ggml_model_exists():
            raise RuntimeError(
                f"Zorunlu model dosyasÄ± bulunamadÄ±: {self.ggml_model_path.resolve()}\n"
                "LÃ¼tfen 'ggml-large-v3-turbo.bin' dosyasÄ±nÄ±n 'models' klasÃ¶rÃ¼nde olduÄŸundan emin olun. "
                "Program baÅŸka bir model denemeyecek veya indirme yapmayacaktÄ±r."
            )

        # 2. GGML modelini yÃ¼klemeyi dene
        self.log_info("GGML modeli bulundu, pywhispercpp ile yÃ¼kleniyor...")
        try:
            self.model = self._load_ggml_model()
            self.log_info(f"GGML modeli '{self.ggml_model_path.name}' baÅŸarÄ±yla yÃ¼klendi.")
            self.log_info("Sistem sadece bu modeli kullanacak ÅŸekilde yapÄ±landÄ±rÄ±ldÄ±.")
        except Exception as e:
            self.log_error(f"GGML modeli yÃ¼klenirken kritik bir hata oluÅŸtu: {e}", exc_info=True)
            raise RuntimeError(
                f"GGML modeli '{self.ggml_model_path.name}' bulundu ancak yÃ¼klenemedi. "
                "DosyanÄ±n bozuk olmadÄ±ÄŸÄ±nÄ± veya pywhispercpp ile uyumlu olduÄŸunu kontrol edin. "
                "Program devam edemiyor."
            )

    def _load_ggml_model(self) -> Optional[Any]:
        """GGML modelini yÃ¼kle"""
        if not PYWHISPERCPP_AVAILABLE or WhisperCppModel is None:
            self.log_error("pywhispercpp kÃ¼tÃ¼phanesi bulunamadÄ±!")
            return None
            
        default_params = self.whisper_config.get("pywhispercpp_params", {})
        self.log_info(f"pywhispercpp iÃ§in kullanÄ±lan model baÅŸlatma parametreleri: {default_params}")
        # Model'i baÅŸlat (dile dair parametreler transcribe sÄ±rasÄ±nda verilecek)
        model = WhisperCppModel(str(self.ggml_model_path.resolve()), **default_params)
        return model

    def transcribe_audio(self, audio_file: str) -> Dict:
        """
        Ses dosyasÄ±nÄ± transkript et
        
        Args:
            audio_file: Transkript edilecek ses dosyasÄ± yolu
            
        Returns:
            Transkripsiyon sonucu
        """
        try:
            # CPU monitÃ¶rÃ¼nÃ¼ baÅŸlat
            self._start_cpu_monitor()
            
            # Model tipini daha gÃ¼venilir ÅŸekilde belirle
            is_ggml_model = self.use_ggml_model and isinstance(self.model, WhisperCppModel)
            
            if is_ggml_model:
                self.log_info("GGML (pywhispercpp) modeli ile transkripsiyon yapÄ±lÄ±yor.")
                
                # pywhispercpp parametrelerini hazÄ±rla
                params = self.whisper_config.get("pywhispercpp_transcribe_params", {}).copy()
                
                # pywhispercpp desteklemeyen parametreleri kaldÄ±r
                unsupported_params = ["word_timestamps", "response_format"]
                for param in unsupported_params:
                    if param in params:
                        self.log_info(f"pywhispercpp desteklemiyor, parametre kaldÄ±rÄ±ldÄ±: {param}")
                        params.pop(param)
                
                # TÃ¼rkÃ§e dilini zorla ve Ã§eviriyi kapat
                params["language"] = "tr"
                params["translate"] = False
                
                # CPU kullanÄ±mÄ±na gÃ¶re optimal thread ve processor sayÄ±sÄ±nÄ± ayarla
                if self.auto_adjust_threads:
                    optimal_threads = self._get_optimal_thread_count()
                    optimal_processors = max(2, optimal_threads // 2)
                    params["n_processors"] = optimal_processors
                    self.log_info(f"ğŸ¯ CPU kontrolÃ¼: n_processors={optimal_processors} (limit: %{self.cpu_limit*100:.0f})")
                
                # Ã‡ok iÅŸlem (paralel process) sayÄ±sÄ±nÄ± parametrelerden Ã§ek, yoksa CPU Ã§ekirdek sayÄ±sÄ±nÄ± kullan
                n_processors = params.pop("n_processors", os.cpu_count())
                self.log_info(f"pywhispercpp transkripsiyon parametreleri: {params}, n_processors={n_processors}")
                
                result = self.model.transcribe(audio_file, n_processors=n_processors, **params)
            else:
                self.log_info("OpenAI Whisper (Pytorch) modeli ile transkripsiyon yapÄ±lÄ±yor.")
                if self.model is None:
                    self.log_error("Model yÃ¼klenmemiÅŸ!")
                    return {"text": "", "segments": [], "error": "Model not loaded"}
                    
                result = self.model.transcribe(
                    audio_file,
                    language=self.language,
                    task=self.task,
                    temperature=self.temperature,
                    fp16=False  # CPU kullanÄ±mÄ± iÃ§in
                )
            
            self.log_info("Transkripsiyon tamamlandÄ±.")
            model_name = "GGML" if is_ggml_model else "OpenAI Whisper"
            return self._process_transcription_result(result, model_name)
            
        except Exception as e:
            self.log_error(f"Transkripsiyon sÄ±rasÄ±nda hata: {e}", exc_info=True)
            return {"text": "", "segments": [], "error": str(e)}
        finally:
            # CPU monitÃ¶rÃ¼nÃ¼ durdur
            self._stop_cpu_monitor()

    def _process_transcription_result(self, result, model_name):
        """Transkripsiyon sonucunu iÅŸle ve standardize et"""
        processed_result = {
            'text': '',
            'segments': [],
            'language': 'tr',
            'model_used': model_name,
            'word_confidence': [],
            'hallucination_risk': 0.0,
            'confidence_stats': {}
        }
        
        # Model tipini gÃ¼venilir biÃ§imde tespit et
        is_ggml_model = isinstance(self.model, WhisperCppModel)
        
        if is_ggml_model:
            def _seg_to_dict(seg):
                return {
                    'start': getattr(seg, 'start', 0.0),
                    'end': getattr(seg, 'end', 0.0),
                    'text': getattr(seg, 'text', str(seg))
                }

            if isinstance(result, list):
                # Segment listesi -> dict'e dÃ¶nÃ¼ÅŸtÃ¼r
                processed_result['segments'] = [_seg_to_dict(s) for s in result]
                processed_result['text'] = " ".join(seg['text'] for seg in processed_result['segments'])
            elif hasattr(result, 'text'):
                processed_result['text'] = result.text
                if hasattr(result, 'segments'):
                    processed_result['segments'] = [_seg_to_dict(s) for s in result.segments]
                else:
                    processed_result['segments'] = [_seg_to_dict(result)]
        else:
            # OpenAI Whisper formatÄ±
            processed_result['text'] = result.get('text', '')
            processed_result['segments'] = result.get('segments', [])
            processed_result['language'] = result.get('language', 'tr')
            
            # Word-level confidence extraction (sadece OpenAI Whisper iÃ§in)
            self._extract_word_confidence(result, processed_result)
        
        # Hallucination detection (her iki model iÃ§in)
        hallucination_analysis = self._detect_hallucination(processed_result['segments'])
        processed_result['hallucination_risk'] = hallucination_analysis.get('risk_score', 0.0)
        processed_result['hallucination_analysis'] = hallucination_analysis
        
        return processed_result
    
    def _get_optimal_thread_count(self) -> int:
        """CPU kullanÄ±mÄ±na gÃ¶re optimal thread sayÄ±sÄ± hesapla"""
        cpu_count = os.cpu_count() or 4
        cpu_percent = psutil.cpu_percent(interval=1.0)
        
        if cpu_percent > self.cpu_limit * 100:
            # CPU kullanÄ±mÄ± yÃ¼ksekse thread sayÄ±sÄ±nÄ± azalt
            optimal_threads = max(2, int(cpu_count * 0.5))
            self.log_info(f"CPU kullanÄ±mÄ± yÃ¼ksek (%{cpu_percent:.1f}), thread sayÄ±sÄ± azaltÄ±ldÄ±: {optimal_threads}")
        else:
            # Normal durumda CPU limitine gÃ¶re ayarla
            optimal_threads = max(2, int(cpu_count * self.cpu_limit))
            
        return optimal_threads
    
    def _start_cpu_monitor(self):
        """CPU kullanÄ±m monitÃ¶rÃ¼nÃ¼ baÅŸlat"""
        if not self.auto_adjust_threads or self.cpu_monitor_active:
            return
            
        self.cpu_monitor_active = True
        
        def monitor_cpu():
            while self.cpu_monitor_active:
                try:
                    cpu_percent = psutil.cpu_percent(interval=5.0)
                    if cpu_percent > self.cpu_limit * 100:
                        self.log_info(f"âš ï¸ CPU kullanÄ±mÄ± limit aÅŸÄ±ldÄ±: %{cpu_percent:.1f} (limit: %{self.cpu_limit*100:.0f})")
                    time.sleep(10)
                except Exception as e:
                    self.log_error(f"CPU monitÃ¶r hatasÄ±: {e}")
                    break
        
        monitor_thread = threading.Thread(target=monitor_cpu, daemon=True)
        monitor_thread.start()
        self.log_info(f"CPU monitÃ¶rÃ¼ baÅŸlatÄ±ldÄ± (limit: %{self.cpu_limit*100:.0f})")
    
    def _stop_cpu_monitor(self):
        """CPU monitÃ¶rÃ¼nÃ¼ durdur"""
        self.cpu_monitor_active = False
    
    def _extract_word_confidence(self, result, processed_result):
        """OpenAI Whisper sonuÃ§larÄ±ndan word-level confidence Ã§Ä±kar"""
        word_confidence = []
        
        for segment in result.get('segments', []):
            if 'words' in segment:
                for word_info in segment['words']:
                    word_confidence.append({
                        'word': word_info.get('word', '').strip(),
                        'start': word_info.get('start', 0.0),
                        'end': word_info.get('end', 0.0),
                        'probability': word_info.get('probability', 0.5)
                    })
        
        processed_result['word_confidence'] = word_confidence
        
        # Confidence istatistikleri
        if word_confidence:
            confidences = [w['probability'] for w in word_confidence]
            processed_result['confidence_stats'] = {
                'mean': sum(confidences) / len(confidences),
                'min': min(confidences),
                'max': max(confidences),
                'low_confidence_count': len([c for c in confidences if c < 0.3])
            }
    
    def transcribe_with_speaker_diarization(self, audio_file: str, diarization_result: Dict) -> Dict:
        """
        KonuÅŸmacÄ± bilgileri ile birlikte transkript et
        
        Args:
            audio_file: Ses dosyasÄ± yolu
            diarization_result: Speaker diarization sonucu
            
        Returns:
            KonuÅŸmacÄ± etiketli transkripsiyon sonucu
        """
        # Normal transkripsiyon yap
        transcription_result = self.transcribe_audio(audio_file)
        
        if transcription_result.get("error"):
            return transcription_result
        
        # KonuÅŸmacÄ± etiketlerini ekle
        segments_with_speakers = []
        
        for segment in transcription_result.get("segments", []):
            start_time = segment.get("start", 0.0)
            end_time = segment.get("end", 0.0)
            
            # Bu segment iÃ§in konuÅŸmacÄ±yÄ± bul
            speaker = self._find_speaker_for_segment(start_time, end_time, diarization_result)
            
            segment_with_speaker = segment.copy()
            segment_with_speaker["speaker"] = speaker
            segments_with_speakers.append(segment_with_speaker)
        
        # SonuÃ§larÄ± gÃ¼ncelle
        transcription_result["segments"] = segments_with_speakers
        
        # KonuÅŸmacÄ±lÄ± metni oluÅŸtur
        speaker_labeled_text = ""
        current_speaker = None
        
        for segment in segments_with_speakers:
            speaker = segment.get("speaker", "UNKNOWN")
            text = segment.get("text", "")
            
            if speaker != current_speaker:
                if speaker_labeled_text:
                    speaker_labeled_text += "\n\n"
                speaker_labeled_text += f"[{speaker}]: "
                current_speaker = speaker
            
            speaker_labeled_text += text + " "
        
        transcription_result["speaker_labeled_text"] = speaker_labeled_text.strip()
        
        # YalnÄ±zca ana konuÅŸmacÄ±nÄ±n transkripsiyonunu al
        main_speaker = diarization_result.get("main_speaker")
        main_segments = [seg for seg in segments_with_speakers if seg.get("speaker") == main_speaker]
        transcription_result["segments"] = main_segments
        # Ana konuÅŸmacÄ±nÄ±n birleÅŸtirilmiÅŸ metni
        transcription_result["text"] = " ".join(seg.get("text", "") for seg in main_segments).strip()
        
        return transcription_result
    
    def _find_speaker_for_segment(self, start_time: float, end_time: float, diarization_result: Dict) -> str:
        """
        Segment iÃ§in en uygun konuÅŸmacÄ±yÄ± bul
        
        Args:
            start_time: Segment baÅŸlangÄ±Ã§ zamanÄ±
            end_time: Segment bitiÅŸ zamanÄ±
            diarization_result: KonuÅŸmacÄ± ayÄ±rma sonucu
            
        Returns:
            KonuÅŸmacÄ± adÄ±
        """
        segment_duration = end_time - start_time
        speaker_durations = {}
        
        # Her konuÅŸmacÄ± iÃ§in overlap sÃ¼resini hesapla
        for diar_segment in diarization_result.get("segments", []):
            diar_start = diar_segment["start_time"]
            diar_end = diar_segment["end_time"]
            speaker = diar_segment["speaker"]
            
            # Overlap hesapla
            overlap_start = max(start_time, diar_start)
            overlap_end = min(end_time, diar_end)
            overlap_duration = max(0, overlap_end - overlap_start)
            
            if speaker not in speaker_durations:
                speaker_durations[speaker] = 0
            speaker_durations[speaker] += overlap_duration
        
        # En Ã§ok overlap olan konuÅŸmacÄ±yÄ± dÃ¶ndÃ¼r
        if speaker_durations:
            return max(speaker_durations.items(), key=lambda x: x[1])[0]
        
        return "UNKNOWN"
    
    def get_model_info(self) -> Dict:
        """Model bilgilerini al"""
        return {
            "model_size": self.model_size,
            "device": self.device,
            "compute_type": "GGML" if self.model is None else "OpenAI Whisper",
            "language": self.language,
            "task": self.task,
            "is_loaded": self.model is not None
        }
    
    def format_for_editing(self, transcription_result: Dict) -> str:
        """
        DÃ¼zenleme iÃ§in formatlanmÄ±ÅŸ metni dÃ¶ndÃ¼r.
        KonuÅŸmacÄ± etiketli metin varsa onu, yoksa ham metni kullan.
        """
        # EÄŸer konuÅŸmacÄ± etiketli metin varsa kullan
        if transcription_result.get("speaker_labeled_text"):
            return transcription_result.get("speaker_labeled_text")
        # Aksi halde dÃ¼z metni dÃ¶ndÃ¼r
        return transcription_result.get("text", "")

    def _detect_hallucination(self, segments: List[Dict]) -> Dict:
        """Åimdilik her zaman 'halÃ¼sinasyon yok' sonucu dÃ¶ndÃ¼ren sade fonksiyon."""
        return {"risk_score": 0.0, "details": []}

    def detect_hallucinations(self, segments: List[Dict], language: str) -> List[Dict]:
        """Transkripsiyon segmentlerinde olasÄ± halÃ¼sinasyonlarÄ± tespit eder."""
        try:
            from whisper_normalizer.indic_normalizer import IndicNormalizer
            from whisper_normalizer.english_normalizer import EnglishNormalizer
            
            if language == "tr":
                # TÃ¼rkÃ§e iÃ§in Ã¶zel normalleÅŸtirici gerekebilir, ÅŸimdilik temel karakterler
                normalizer = lambda x: x
            elif language in ["en", "english"]:
                normalizer = EnglishNormalizer()
            else:
                 # DiÄŸer diller iÃ§in genel bir normalleÅŸtirici
                normalizer = IndicNormalizer()

            hallucinations = []
            for segment in segments:
                # Segment nesnesinden metni doÄŸru al
                text = segment.text if hasattr(segment, 'text') else segment.get('text', '')
                normalized_text = normalizer(text)
                
                # Basit halÃ¼sinasyon tespiti kurallarÄ± (Ã¶rnek)
                # ... (buraya daha karmaÅŸÄ±k kurallar eklenebilir)
                
            return hallucinations
            
        except ImportError:
            self.log_warning("HalÃ¼sinasyon tespiti iÃ§in 'whisper_normalizer' kurulu deÄŸil, atlanÄ±yor.")
            return []
        except Exception as e:
            self.log_error(f"Hallucination tespiti hatasÄ±: {e}")
            return []

    def _apply_language_rules(self, transcription_result: Dict) -> Dict:
        """TÃ¼rkÃ§e metin Ã¼zerinde basit dil temizliÄŸi yapabilir.
        Åimdilik doÄŸrudan gelen nesneyi dÃ¶ndÃ¼rÃ¼r."""
        return transcription_result 